\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\title{Neukirch Reading Notes}
\author{Cecilia Chan}
\date{June 2021}
\DeclareMathOperator{\fractionfield}{frac}
\DeclareMathOperator{\closure}{closure}
\begin{document}
\maketitle

\section*{Page 7}
After we obtained the relation $ \det(bE - (a_{ij}))\omega_i = 0 $, we knew $ \omega_i \ne 0 $ because it is in the generating set, but we cannot conclude $ \det(bE - (a_{ij}))\omega_i = 0 $ because of the possibility of zero divisor. (Recall $ A[b_1 \cdots b_n] $ is just a ring, no requirement for it to be an integral domain).

Since $ \omega_i $ is a generating set, we can use it to generate 1. $ 1 = \sum c_i \omega_i $. We are sure $ 1 \ne 0 $ because $ A[b_1 \cdots b_n ] $ cannot be the zero ring. Now multiply both sides by $ \det(bE - (a_{ij})) $. Now either $ \det(bE - (a_{ij})) = 0 $ so that both side is 0, or it isn't so that the left hand side is not zero but the right hand side is zero. That concludes $ \det(bE - (a_{ij})) = 0 $

In the last paragraph, it was written it is immediate ..., that is not obvious at all, especially to someone who is not familiar with the terminologies just introduced.

The goal is to prove that the "integral closure" of $ A $ over $ B $ is "integrally closed" in $ B $. Proposition 2.4 involves the use of "integral over". So the whole proof is really a soup of all these terms.

To make it easier to understand, instead of using just an overbar, I write out the closure explicitly. $ \closure(P|Q) $ means the integral closure of $ P $ over $ Q $.

There is a very easy lemma that we can start with:

$ \closure(A | B) $ is integral over $ A $.

This is really just reading the definitions. The closure of $ A $ over $ B $ is all the elements of $ b $ that is integral over $ A $, which is required for the definition of integral over.

$ \closure(A | B) \subseteq B $.

This is just as easy - the definition of closure requires the elements comes from $ B $.

Now, let's examine the statement we wanted to prove. $ \closure(A | B) $ is integrally closed in $ B $, that means:

$ \closure(\closure(A | B) | B) = \closure(A | B) $.

To prevent a notation soup, let $ C = \closure(A | B) $ and $ D = \closure(C | B) $, now our goal becomes proving $ C = D $.

Using our lemmas, we see $ C $ is integral over $ A $, $ C \subseteq B $, $ D $ is integral over $ C $, $ D \subseteq B $. Invoking proposition 2.4, we see $ D $ is integral over $ A $.

Now we want to argue set equality, to do that, we see 
$ C \subseteq D $ because elements in $ C $ must be integral in $ C $ so they are contained in $ D $ as the integral closure of $ C $ over $ B $.

$ D \subseteq C $ because elements in $ D $ must be integral over $ A $, but $ C $ contains all such elements.

That concludes the proof of $ C = D $ and therefore the statement that the integral closure of $ A $ over $ B $ is integrally closed in $ B $.

Not intermediate at all!

\section*{Page 8}
To prove $ B $ is integrally closed, we need to prove

$ \closure(B|\fractionfield(B)) = B $

Let $ C = \closure(B|\fractionfield(B)) $, now $ C $ is integral over $ B $, $ B $ is integral over $ A $ because $ B = \closure(A|L) $. Proposition 2.4 gives $ C $ is integral over $ A $. As above, we claim $ B \subseteq C $ and then $ C \subseteq B $ because elements in $ C$ must be integral over $ A $ but $ B $ contains all such elements.

As $ L | K $ is a finite extension, the set of vectors $ 1, \beta, \beta^2, \cdots, \beta^n $ must be linearly dependent, and that why we can claim the polynomial is 0.

The "hence the same holds for all coefficients" is tricky. The coefficients are sum and products of the roots, so if the roots are integral, so are the coefficients.

$ T_x $ is a linear transformation, to see that, we have:
\begin{eqnarray*}
  & & T_x(\alpha + \beta) \\
  &=& x(\alpha + \beta)   \\
  &=& x\alpha + x\beta    \\
  &=& T_x(\alpha) + T_x(\beta)
\end{eqnarray*}
And also:
\begin{eqnarray*}
  & & a T_x(\alpha) \\
  &=& ax\alpha      \\
  &=& xa\alpha      \\
  &=& T_x(a\alpha)
\end{eqnarray*}
Now it makes sense to talk about the trace and determinant of the matrix representation of this linear transformation. The definition is independent of basis because trace and determinant are the same for similar matrices.

\section*{Page 9}
The characteristic polynomial is defined as $ \det(tI - T_x) $ is because the coefficient of $ t^n $ will always be 1.

The trace is a homomorphism from the addition group of $ L $ to the addition group of $ K $.

The norm is a homomorphism from the multiplication group of $ L $ to the addition group of $ K $. The $ L^* $ is actually $ L^{\times} $, the groups of units.

Consider $ y = \sum_{i=1}^{d}\sum_{j=0}^{m-1}d_{ij}\alpha_i x^j $, then we have

\begin{eqnarray*}
  & & T_x(y) \\
  &=& x(\sum_{i=1}^{d}\sum_{j=0}^{m-1}d_{ij}\alpha_i x^j) \\
  &=& \sum_{i=1}^{d}\sum_{j=0}^{m-1}d_{ij}\alpha_i x^{j+1} \\
  &=& \sum_{i=1}^{d}\sum_{j=1}^{m}d_{i(j-1)}\alpha_i x^j \\
  &=& \sum_{i=1}^{d}(\sum_{j=1}^{m-1}d_{i(j-1)}\alpha_i x^j + d_{i(m-1)}\alpha_i x^m) \\
  &=& \sum_{i=1}^{d}(\sum_{j=1}^{m-1}d_{i(j-1)}\alpha_i x^j - d_{i(m-1)}\alpha_i(\sum_{j=0}^{m-1}c_{m-j}x^j)) \\
  &=& \sum_{i=1}^{d}(\sum_{j=1}^{m-1}d_{i(j-1)}\alpha_i x^j - d_{i(m-1)}\alpha_i(\sum_{j=1}^{m-1}c_{m-j}x^j) - d_{i(m-1)}\alpha_ic_m) \\
  &=& \sum_{i=1}^{d}(\sum_{j=1}^{m-1}(d_{i(j-1)} - d_{i(m-1)}c_{m-j}) \alpha_i x^j - d_{i(m-1)}\alpha_ic_m) \\
\end{eqnarray*}

To make sense out of the matrix expression in the book, the transformation is actually encoded as multiplying the row vector on the left. That will match with the expression above.

\section*{Page 10}
To understand why the characteristic polynomial of the transformation is the minimal polynomial, we consider the Laplace expansion on the last row, if could claim the determinant of the submatrix corresponding to the coefficient is a power of $ t $ with proper sign, then we are done.

To see why the submatrix's determinant is as such, we consider an example by octave code as follow:

\begin{verbatim}
clear
clc
t = 3
n = 5
source = [t * eye(n,n) zeros(n, 1)] + [zeros(n, 1) (-1 * eye(n,n))];
for i = 1:n
  matrix = source;
  matrix(:,[i]) = []
  det(matrix)
end 
\end{verbatim}

and the result as follow:

\begin{verbatim}
t = 3
n = 5
matrix =

  -1   0   0   0   0
   3  -1   0   0   0
   0   3  -1   0   0
   0   0   3  -1   0
   0   0   0   3  -1

ans = -1
matrix =

   3   0   0   0   0
   0  -1   0   0   0
   0   3  -1   0   0
   0   0   3  -1   0
   0   0   0   3  -1

ans = 3
matrix =

   3  -1   0   0   0
   0   3   0   0   0
   0   0  -1   0   0
   0   0   3  -1   0
   0   0   0   3  -1

ans = -9
matrix =

   3  -1   0   0   0
   0   3  -1   0   0
   0   0   3   0   0
   0   0   0  -1   0
   0   0   0   3  -1

ans = 27
matrix =

   3  -1   0   0   0
   0   3  -1   0   0
   0   0   3  -1   0
   0   0   0   3   0
   0   0   0   0  -1

ans = -81
\end{verbatim}

Now it is fairly obvious why, the determinant is simply the product of the diagonal, this is because either the right hand side of -1 or the below of the 3 is always a series of zero, making the Laplace expansion trivial. It is a power because we get more and more 3 as we eliminate the column further right, and the sign is right because we are eliminating one -1 at a time make the sign flips just right.

$ x $ is a root of $ p(x) $, therefore $ \sigma(x) $ must map $ x $ to one of the $ m $ root of $ p(x) $. The conclude why we have $ m $ equivalence classes.

Let assume $ L = K(x)(y) $, now the minimal polynomial $ g $ of $ y $ over $ K(x) $ has degree $ d $, therefore each group has $ d $ element, each element maps $ y $ to a different root of $ g $ in $ \overline{K} $.

The fact that $ L = K(x)(y) $ should come from the fundamental theorem of Galois theory (so that we claim $ L | K(x) $ is separable given $ L | K $ is separable and the primitive element theorem (so that a separable extension is a simple extension). I still need to read through those make sure I understand. For now I assume I know.

\end{document}
